{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Capítulo 16: Natural language processing with RNNs and attention (página 525)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una aproximación común al realizar NLP es utilizar redes neuronales recurrentes (RNN)\r\n",
    "En este primer ejercicio haremos unpredictor de caracteres (qué caracter iría después de otro probablemente)\r\n",
    "RNN stateless --> Aprende de posiciones aleatorias de texto en cada iteración sin información alguna del resto del texto.\r\n",
    "RNN stateful --> Que preserva un \"estado\" oculto a través de las iteraciones de entrenamiento y continúa leyendo donde lo dejó previamente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importando nuestros paquetes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creando el dataset de entrenamiento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primero, descargamos el dataset del trabajo de Shakespeare's utilizando Keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\" # URL recortada\r\n",
    "filepath = keras.utils.get_file('Shakespeare.txt', shakespeare_url)\r\n",
    "with open(filepath) as f:\r\n",
    "    shakespeare_text = f.read()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora que ya tenemos el texto cargado en memoria, necesitamos tokenizarlo, esto significa que daremos un valor numérico a cada caracter único presente en nuestro set de datos. Para ello, utilizaremos Tokenizer de Keras."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\r\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Establecemos `char_level=True` para obtener los tokens a nivel de caracter y no de palabra, como lo hace ya por defecto la función.\r\n",
    "Nota que tokenizer convierte el texto a minúsculas por defecto, pero puedes utilizar el argumento `lower=True` para evitar este comportamiento."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora, el tokenizador puede codificar un enunciado (o una lista de enunciados) a una lista de IDs de caracteres y viceversa, y entonces nos puede decir que tantos caracteres distintos hay y el total de caracteres distintos en el texto."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejemplo del uso de tokenizer:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "max_id = len(tokenizer.word_index) # total de caracteres distintos\r\n",
    "max_id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "dataset_size = tokenizer.document_count # total de caracteres en el documento\r\n",
    "dataset_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a codificar el texto completo así, cada caracter es representado por si propio ID (restaremos 1 a cada ID para obtener IDs del 0 al 38 en lugar del 1 al 39):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\r\n",
    "encoded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dividamos nuestro dataset en un set de entrenamiento y otro de prueba"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es muy importante evitar cualquier traslape entre nuestro set de entrenamiento, el set de validación y el set de prueba. Por ejemplo, podemos tomar el primer 90% de el texto para el set de entrenamiento, el siguiente 5% para el set de validación y el resto para el set de prueba."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entonces, tomamos el 90% del dataset como set de entrenamiento y el resto para prueba y validación, también creamos una instancia de tf.data.Dataset que retornará cada caracter, uno por uno de este set:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_size = dataset_size * 90 // 100\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Espacio para explicar el uso de encoded**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora que tenemos nuestro set de entrenamiento, este consiste en una única secuencia de más de un millón de caracteres; no podemos entrenar nuestra red neuronal directamente sobre este set: nuestra RNN sería equivalente a un red profunda con más de un millón de capas, y tendríamos una sola instancia (pero bastante larga) sobre la cual entrenar (nuestras entradas)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En lugar de esto, utilizaremos el método `window()` del dataset, que convertirá esta larga secuencia de caracteres en pequeñas ventanas (muestras) de texto. Cada instancia en el dataset será una subcadena del texto completo, y la RNN será entrenada en cada ocasión únicamente con los caracteres de la muestra. Esto es llamado ***truncated backprogation through time***. Utilicemos el método `window()` sobre el dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "n_steps = 100\r\n",
    "window_length = n_steps + 1 # objetivo = entrada desplazada 1 carácter adelante\r\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Nota: Puedes intentar ir cambiando n_steps: es más fácil entrenar RNNs con secuencias cortas de caracteres pero, por supuesto, la RNN no será capaz de aprender ningún patrón de una longitud más grande que n_steps, así que procura no hacerlo demasiado corto***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por defecto, el método `window()` genera ventanas que no se traslapan, pero para obtener el set de entrenamiento más largo posible utilizamos `shift=1`, de esta manera, la primera ventana contendrá los caracteres del 0 al 100 (100 porque es el número al que hemos igualado n_steps), la segunda contendrá los caracteres del 1 al 101, y así sucesivamente. Para asegurarnos que todas las ventanas sean de exactamente 101 caracteres (lo cual nos permitirá crear batches sin tener que agregar algún tipo de relleno), establecemos `drop_remainder=True`(de otra manera, las últimas 100 ventanas contendrán 100 caracteres, 99 caracteres, y así sucesivamente hasta llegar a 1 caracter)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convirtiendo datasets anidados a tensores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "EL método `window()` generará un dataset que contiene ventanas, donde cada una de ellas es representada igualmente como un dataset. Es un dataset anidado, análogo a una lista de listas. Esto es útil cuando queremos transformar las ventanas utilizando los métodos de dataset (por ejemplo, para intercambiarlos o muestrearlos). Sin embargo, no podemos utilizar un dataset anidado directamente, dado que nuestro modelo espera tensores, no datasets. Por ello, es necesario llamar al método `flat_map()`, este convierte un *dataset anidado* en un *dataset plano*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note que utilizamos la variable `windows_length` dado que todas las ventanas tienen la misma longitud: dado que todas las ventanas tienen exactamente la misma longitud, obtendremos un único tensor por cada una de ellas. Ahora, el dataset contendrá ventanas consecutivas de 101 caracteres cada una."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mientras que el descenso de gradiente funciona mejor cuando las instancias en el set de entrenamiento son independientes y distribuidas de manera uniforme, necesitamos reordenar esas ventanas. Solo así podremos generar muestras de esas ventanas y separar las entradas (los primeros 100 caracteres) del objetivo (el último caracter)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "batch_size = 32\r\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\r\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Las variables categóricas suelen ser codificadas, generalmente como vectores one-hot o como embeddings. Aquí, codificaremos cada caracter utilizando un vector one-hot porque tenemos realmente pocos caracteres únicos (solo 39)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "dataset = dataset.map(\r\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\r\n",
    ")\r\n",
    "\r\n",
    "# Finalmente, necesitamos únicamente añadir precarga:\r\n",
    "dataset = dataset.prefetch(1)\r\n",
    "dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construyendo y entrenando el modelo Char-NN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para predecir el siguiente caracter basado en los 100 anteriores, podemos utilizar una Red Neuronal Recurrente (RNN) con dos capas GRU de 128 unidades cada una y 20% de dropout para ambas entradas (dropout) y los estados ocultos (recurrent_dropout). La capa de salida es una capa Dense distribuida-temporalmente. Esta vez, la capa debe tener 39 unidades (max_id) porque hay 39 caracteres distintos en el texto, y deseamos obtener la probabilidad para cada posible caracter (una vez, por cada paso)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model = keras.models.Sequential([\r\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=(None, max_id), dropout=0.2, recurrent_dropout=0.2),\r\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\r\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\r\n",
    "])\r\n",
    "\r\n",
    "#model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\r\n",
    "#history = model.fit(dataset, epochs=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilizando el modelo Char-NN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora que ya tenemos el modelo que puede predecir el siguiente caracter en el texto escrito por Shakespear. Para alimentarlo con algo de texto, primero necesitamos preprocesarlo como hicimos anteriormente, así que crearemos una función para ello:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def preprocess(texts):\r\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\r\n",
    "    return tf.one_hot(X, max_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Probemos nuestro modelo para predecir el siguiente caracter de una frase típica:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "X_new = preprocess([\"How are \"])\r\n",
    "Y_pred = model.predict_classes(X_new)\r\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][1]\r\n",
    "print(Y_pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[17 17  6 28 17 19 19 28]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\omard\\.virtualenvs\\tensorflowCertification-g0xU6U4H\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('tensorflowCertification-g0xU6U4H': pipenv)"
  },
  "interpreter": {
   "hash": "5a0752da3053b84f5c190f28b54aad6c7460a88b6573662b0727913cd6c3acb6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}