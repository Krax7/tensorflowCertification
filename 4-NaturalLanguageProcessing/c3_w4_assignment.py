# -*- coding: utf-8 -*-
"""C3_W4_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W4/assignment/C3_W4_Assignment.ipynb
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""**Note:** This notebook can run using TensorFlow 2.5.0"""

#!pip install tensorflow==2.5.0

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
### YOUR CODE HERE
# Figure out how to import regularizers
from tensorflow.keras import regularizers
###
import tensorflow.keras.utils as ku 
import numpy as np

tokenizer = Tokenizer()

# sonnets.txt
#!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K

data = open('./data/sonnets.txt').read()

corpus = data.lower().split("\n")


tokenizer.fit_on_texts(corpus) # Obtenemos los tokens para todas las palabras en el archivo
total_words = len(tokenizer.word_index) + 1 # A partir de aquí, ya tenemos un índice de palabra con su token asociado

# create input sequences using list of tokens
input_sequences = []
for line in corpus:
	# Aquí convertimos cada texto a secuencias de texto, básicamente hacemos listas de palabras dentro de una lista más
	# grande pero reemplazando las palabras por tokens
	token_list = tokenizer.texts_to_sequences([line])[0]
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[:i+1]
		input_sequences.append(n_gram_sequence) # Aquí obtenemos todas las longitudes posibles para cada línea, de tal
		# manera que podamos entrenar con todas las variantes, por ejemplo:
		"""
		Inicial: Hola a todos
		Tamaño 2: Ho
		Tamaño 3: Hol
		Tamaño 4: Hola
		...
		"""

# pad sequences 
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# create predictors and label
predictors, label = input_sequences[:,:-1],input_sequences[:,-1]

label = ku.to_categorical(label, num_classes=total_words)

### START CODE HERE
model = Sequential()
"""
Embedding Layer

Turns positive integers (indexes) into dense vectors of fixed size.

e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]

This layer can only be used as the first layer in a model.

Arguments
input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.
output_dim: Integer. Dimension of the dense embedding.
input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect
Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).

Input shape
2D tensor with shape: (batch_size, input_length).

Output shape
3D tensor with shape: (batch_size, input_length, output_dim).
"""
model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) # Segundo arg: dimensionalidad del embedding, arbitrario.
model.add(Bidirectional(LSTM(150, return_sequences=True)))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dense(total_words/2,
				kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
				bias_regularizer=regularizers.l2(1e-4),
				activity_regularizer=regularizers.l2(1e-5),
				activation='relu'))
model.add(Dense(total_words, activation='softmax'))
# Pick an optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
### END CODE HERE

print(model.summary())

history = model.fit(predictors, label, epochs=100, verbose=1)

import matplotlib.pyplot as plt
acc = history.history['accuracy']
loss = history.history['loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training accuracy')
plt.title('Training accuracy')

plt.figure()

plt.plot(epochs, loss, 'b', label='Training Loss')
plt.title('Training loss')
plt.legend()

plt.show()

seed_text = "Help me Obi Wan Kenobi, you're my only hope"
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
print(seed_text)